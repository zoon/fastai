{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hide_input": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: en_core_web_sm==2.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz#egg=en_core_web_sm==2.0.0 in /home/fastai/anaconda3/envs/fastai/lib/python3.6/site-packages (2.0.0)\n",
      "\n",
      "\u001b[93m    Linking successful\u001b[0m\n",
      "    /home/fastai/anaconda3/envs/fastai/lib/python3.6/site-packages/en_core_web_sm\n",
      "    -->\n",
      "    /home/fastai/anaconda3/envs/fastai/lib/python3.6/site-packages/spacy/data/en_core_web_sm\n",
      "\n",
      "    You can now load the model via spacy.load('en_core_web_sm')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from fastai.nlp import *\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from IPython.display import display\n",
    "\n",
    "from sklearn import metrics\n",
    "import feather\n",
    "import pdpbox.pdp as pdp\n",
    "from plotnine import *\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "import os\n",
    "from contextlib import contextmanager\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 6)\n",
    "np.set_printoptions(precision=5)\n",
    "pd.set_option(\"display.precision\", 5)\n",
    "%precision 5\n",
    "\n",
    "\n",
    "import spacy\n",
    "#!python -m spacy download en_core_web_sm\n",
    "\n",
    "# constants #################################################################\n",
    "PATH = 'data/aclImdb/'\n",
    "import pymorton as pm\n",
    "\n",
    "\n",
    "def morton_indexer(w, h):\n",
    "    assert w & (w - 1) == 0\n",
    "    assert h & (h - 1) == 0\n",
    "    enc = np.zeros(w * h, dtype=np.int)\n",
    "    dec = np.zeros(w * h, dtype=np.int)\n",
    "    i = 0\n",
    "    for r in range(h):\n",
    "        for c in range(w):\n",
    "            m = pm.interleave(c, r)\n",
    "            enc[i] = m\n",
    "            dec[m] = i\n",
    "            i += 1\n",
    "    return enc, dec\n",
    "\n",
    "\n",
    "# defines: ##################################################################\n",
    "# @contextmanager\n",
    "# def rf_samples(n='all'):\n",
    "#     if isinstance(n, int) and n > 0:\n",
    "#         set_rf_samples(n)\n",
    "#     try:\n",
    "#         yield\n",
    "#     finally:\n",
    "#         if isinstance(n, int) and n > 0:\n",
    "#             reset_rf_samples()\n",
    "\n",
    "\n",
    "def split_vals(a, n):\n",
    "    return a[:n].copy(), a[n:].copy()\n",
    "\n",
    "\n",
    "class AttrDict(dict):\n",
    "    __getattr__ = dict.__getitem__\n",
    "    __setattr__ = dict.__setitem__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Readme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "hide_input": false
   },
   "source": [
    "```\n",
    "Large Movie Review Dataset v1.0\n",
    "\n",
    "Overview\n",
    "\n",
    "This dataset contains movie reviews along with their associated binary\n",
    "sentiment polarity labels. It is intended to serve as a benchmark for\n",
    "sentiment classification. This document outlines how the dataset was\n",
    "gathered, and how to use the files provided. \n",
    "\n",
    "Dataset \n",
    "\n",
    "The core dataset contains 50,000 reviews split evenly into 25k train\n",
    "and 25k test sets. The overall distribution of labels is balanced (25k\n",
    "pos and 25k neg). We also include an additional 50,000 unlabeled\n",
    "documents for unsupervised learning. \n",
    "\n",
    "In the entire collection, no more than 30 reviews are allowed for any\n",
    "given movie because reviews for the same movie tend to have correlated\n",
    "ratings. Further, the train and test sets contain a disjoint set of\n",
    "movies, so no significant performance is obtained by memorizing\n",
    "movie-unique terms and their associated with observed labels.  In the\n",
    "labeled train/test sets, a negative review has a score <= 4 out of 10,\n",
    "and a positive review has a score >= 7 out of 10. Thus reviews with\n",
    "more neutral ratings are not included in the train/test sets. In the\n",
    "unsupervised set, reviews of any rating are included and there are an\n",
    "even number of reviews > 5 and <= 5.\n",
    "\n",
    "Files\n",
    "\n",
    "There are two top-level directories [train/, test/] corresponding to\n",
    "the training and test sets. Each contains [pos/, neg/] directories for\n",
    "the reviews with binary labels positive and negative. Within these\n",
    "directories, reviews are stored in text files named following the\n",
    "convention [[id]_[rating].txt] where [id] is a unique id and [rating] is\n",
    "the star rating for that review on a 1-10 scale. For example, the file\n",
    "[test/pos/200_8.txt] is the text for a positive-labeled test set\n",
    "example with unique id 200 and star rating 8/10 from IMDb. The\n",
    "[train/unsup/] directory has 0 for all ratings because the ratings are\n",
    "omitted for this portion of the dataset.\n",
    "\n",
    "We also include the IMDb URLs for each review in a separate\n",
    "[urls_[pos, neg, unsup].txt] file. A review with unique id 200 will\n",
    "have its URL on line 200 of this file. Due the ever-changing IMDb, we\n",
    "are unable to link directly to the review, but only to the movie's\n",
    "review page.\n",
    "\n",
    "In addition to the review text files, we include already-tokenized bag\n",
    "of words (BoW) features that were used in our experiments. These \n",
    "are stored in .feat files in the train/test directories. Each .feat\n",
    "file is in LIBSVM format, an ascii sparse-vector format for labeled\n",
    "data.  The feature indices in these files start from 0, and the text\n",
    "tokens corresponding to a feature index is found in [imdb.vocab]. So a\n",
    "line with 0:7 in a .feat file means the first word in [imdb.vocab]\n",
    "(the) appears 7 times in that review.\n",
    "\n",
    "LIBSVM page for details on .feat file format:\n",
    "http://www.csie.ntu.edu.tw/~cjlin/libsvm/\n",
    "\n",
    "We also include [imdbEr.txt] which contains the expected rating for\n",
    "each token in [imdb.vocab] as computed by (Potts, 2011). The expected\n",
    "rating is a good way to get a sense for the average polarity of a word\n",
    "in the dataset.\n",
    "\n",
    "Citing the dataset\n",
    "\n",
    "When using this dataset please cite our ACL 2011 paper which\n",
    "introduces it. This paper also contains classification results which\n",
    "you may want to compare against.\n",
    "\n",
    "\n",
    "@InProceedings{maas-EtAl:2011:ACL-HLT2011,\n",
    "  author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},\n",
    "  title     = {Learning Word Vectors for Sentiment Analysis},\n",
    "  booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},\n",
    "  month     = {June},\n",
    "  year      = {2011},\n",
    "  address   = {Portland, Oregon, USA},\n",
    "  publisher = {Association for Computational Linguistics},\n",
    "  pages     = {142--150},\n",
    "  url       = {http://www.aclweb.org/anthology/P11-1015}\n",
    "}\n",
    "\n",
    "References\n",
    "\n",
    "Potts, Christopher. 2011. On the negativity of negation. In Nan Li and\n",
    "David Lutz, eds., Proceedings of Semantics and Linguistic Theory 20,\n",
    "636-659.\n",
    "\n",
    "Contact\n",
    "\n",
    "For questions/comments/corrections please contact Andrew Maas\n",
    "amaas@cs.stanford.edu\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDB dataset and the sentiment classification taskÂ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hide_input": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 66560\r\n",
      "drwxr-xr-x 4 fastai fastai     4096 Apr  1 09:02 .\r\n",
      "drwxrwxr-x 5 fastai fastai     4096 Mar 31 11:11 ..\r\n",
      "-rw-r--r-- 1 fastai fastai     4037 Jun 26  2011 README\r\n",
      "-rw-r--r-- 1 fastai fastai   845980 Apr 12  2011 imdb.vocab\r\n",
      "-rw-r--r-- 1 fastai fastai   903029 Jun 11  2011 imdbEr.txt\r\n",
      "-rw-rw-r-- 1 fastai fastai 66383276 Apr  1 09:02 pickled.pkl\r\n",
      "drwxr-xr-x 4 fastai fastai     4096 Mar 31 11:11 test\r\n",
      "drwxr-xr-x 5 fastai fastai     4096 Mar 31 11:11 train\r\n"
     ]
    }
   ],
   "source": [
    "!ls -al {PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 66640\r\n",
      "drwxr-xr-x 5 fastai fastai     4096 Mar 31 11:11 \u001b[0m\u001b[01;34m.\u001b[0m/\r\n",
      "drwxr-xr-x 4 fastai fastai     4096 Apr  1 09:02 \u001b[01;34m..\u001b[0m/\r\n",
      "-rw-r--r-- 1 fastai fastai 21021197 Apr 12  2011 labeledBow.feat\r\n",
      "drwxr-xr-x 2 fastai fastai   356352 Mar 31 11:11 \u001b[01;34mneg\u001b[0m/\r\n",
      "drwxr-xr-x 2 fastai fastai   356352 Mar 31 11:11 \u001b[01;34mpos\u001b[0m/\r\n",
      "drwxr-xr-x 2 fastai fastai  1462272 Mar 31 11:11 \u001b[01;34munsup\u001b[0m/\r\n",
      "-rw-r--r-- 1 fastai fastai 41348699 Apr 12  2011 unsupBow.feat\r\n",
      "-rw-r--r-- 1 fastai fastai   612500 Apr 12  2011 urls_neg.txt\r\n",
      "-rw-r--r-- 1 fastai fastai   612500 Apr 12  2011 urls_pos.txt\r\n",
      "-rw-r--r-- 1 fastai fastai  2450000 Apr 12  2011 urls_unsup.txt\r\n"
     ]
    }
   ],
   "source": [
    "ls -al {PATH}train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/aclImdb/train/pos/0_9.txt\r\n",
      "data/aclImdb/train/pos/10000_8.txt\r\n",
      "data/aclImdb/train/pos/10001_10.txt\r\n",
      "data/aclImdb/train/pos/10002_7.txt\r\n",
      "data/aclImdb/train/pos/10003_8.txt\r\n",
      "data/aclImdb/train/pos/10004_8.txt\r\n",
      "data/aclImdb/train/pos/10005_7.txt\r\n",
      "data/aclImdb/train/pos/10006_7.txt\r\n",
      "data/aclImdb/train/pos/10007_7.txt\r\n",
      "data/aclImdb/train/pos/10008_7.txt\r\n",
      "ls: write error: Broken pipe\r\n"
     ]
    }
   ],
   "source": [
    "%ls {PATH}train/pos/*.txt | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 95.8 ms, sys: 39.9 ms, total: 136 ms\n",
      "Wall time: 136 ms\n"
     ]
    }
   ],
   "source": [
    "def load_data():\n",
    "    fname = f'{PATH}pickled.pkl'\n",
    "    if os.path.exists(fname):\n",
    "        with open(fname, 'rb') as f:\n",
    "            return AttrDict(pickle.load(f))\n",
    "    else:\n",
    "        names = ['neg', 'pos']\n",
    "        trn, trn_y = texts_labels_from_folders(f'{PATH}train', names)\n",
    "        val, val_y = texts_labels_from_folders(f'{PATH}test', names)\n",
    "        dta = locals()\n",
    "        with open(fname, 'wb') as f:\n",
    "            pickle.dump(dta, f)\n",
    "        return AttrDict(dta)\n",
    "%time data = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"This is definitely a stupid, bad-taste movie. Eddie Murphy stars in what is written like a sitcom. He is surrounded with his perfect family, full of good family values. If you're looking for politically correct entertainment, this movie is for you. But if you hate the idea of being the only one not to laugh at obscene gags in a movie-theater full of pop-corn addicts, just flee.\",\n",
       " 0)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.trn[0], data.trn_y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_tok = re.compile(f'([{string.punctuation}ââÂ¨Â«Â»Â®Â´Â·ÂºÂ½Â¾Â¿Â¡Â§Â£â¤ââ])')\n",
    "def tokenize(s): return re_tok.sub(r' \\1 ', s).split()\n",
    "\n",
    "# sklearn.feature_extraction.text\n",
    "veczr = CountVectorizer(tokenizer=tokenize)\n",
    "trn_term_doc = veczr.fit_transform(data.trn)\n",
    "val_term_doc = veczr.transform(data.val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\x08\\x08\\x08\\x08a', '\\x10own', '!', '\"', '#', '$', '%', '&', \"'\", '(']"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "veczr.get_feature_names()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['aussie',\n",
       "  'aussies',\n",
       "  'austen',\n",
       "  'austeniana',\n",
       "  'austens',\n",
       "  'auster',\n",
       "  'austere',\n",
       "  'austerity',\n",
       "  'austin',\n",
       "  'austinese'],\n",
       " 1297)"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "veczr.get_feature_names()[5000:5010], veczr.vocabulary_['absurd']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the **log-count ratio  ð**  for each word  *$f$* :\n",
    "\n",
    "$r=\\log \\frac{\\text{ratio of feature $f$ in positive documents}}{\\text{ratio of feature $f$ in negative documents}}$\n",
    "\n",
    "where ratio of feature *$f$* in positive documents is the number of times a positive document has a feature divided by the number of positive documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = trn_term_doc\n",
    "y = data.trn_y\n",
    "\n",
    "\n",
    "def make_pr(x, y):\n",
    "    def pr(y_i):\n",
    "        p = x[y == y_i].sum(0)\n",
    "        return (p + 1) / ((y == y_i).sum() + 1)\n",
    "\n",
    "    return pr\n",
    "\n",
    "\n",
    "pr = make_pr(x, y)\n",
    "\n",
    "r = np.log(pr(1) / pr(0))\n",
    "b = np.log((y == 1).mean() / (y == 0).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81656"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Naive Bayes\n",
    "pre_preds = val_term_doc @ r.T + b\n",
    "preds = pre_preds.T > 0\n",
    "(preds == data.val_y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.83184"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Binarized Naive Bayes\n",
    "x = trn_term_doc.sign()\n",
    "r = np.log(pr(1) / pr(0))\n",
    "\n",
    "pre_preds = val_term_doc.sign() @ r.T + b\n",
    "preds = pre_preds.T > 0\n",
    "(preds == data.val_y).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85128"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = LogisticRegression(C=1e-2, dual=True)\n",
    "m.fit(x, y)\n",
    "preds = m.predict(val_term_doc)\n",
    "\n",
    "(preds==data.val_y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85128"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# binarized\n",
    "m = LogisticRegression(C=1e-2)\n",
    "m.fit(trn_term_doc.sign(), y)\n",
    "preds = m.predict(val_term_doc)\n",
    "\n",
    "(preds==data.val_y).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trigram with NB features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "veczr3 = CountVectorizer(ngram_range=(1,3), tokenizer=tokenize, max_features=800_000)\n",
    "trn_term_doc3 = veczr3.fit_transform(data.trn)\n",
    "val_term_doc3 = veczr3.transform(data.val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 800000)"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_term_doc3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emotional and',\n",
       " 'emotional and complex',\n",
       " 'emotional and physical',\n",
       " 'emotional and powerful',\n",
       " 'emotional and psychological',\n",
       " 'emotional and social',\n",
       " 'emotional and thought',\n",
       " 'emotional appeal',\n",
       " 'emotional as',\n",
       " 'emotional attachment']"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "veczr3.get_feature_names()[200_000:200_010]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.87912"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = data.trn_y\n",
    "x = trn_term_doc3.sign()\n",
    "pr = make_pr(x, y)\n",
    "val_x = val_term_doc3.sign()\n",
    "r = np.log(pr(1)/pr(0))\n",
    "b = np.log((y==1).mean() / (y==0).mean())\n",
    "\n",
    "pre_preds = val_x @ r.T + b\n",
    "preds = pre_preds.T > 0\n",
    "(preds == data.val_y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.905"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = LogisticRegression(C=0.1, dual=True)\n",
    "m.fit(x, y)\n",
    "\n",
    "preds = m.predict(val_x)\n",
    "(preds.T == data.val_y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 800000)"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.91768"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_nb = x.multiply(r)\n",
    "val_x_nb = val_x.multiply(r)\n",
    "m = LogisticRegression(C=0.1, dual=True)\n",
    "m.fit(x_nb, y)\n",
    "preds = m.predict(val_x_nb)\n",
    "(preds.T == data.val_y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25000, 800000), (25000, 800000))"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, x_nb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fastai NBSVM++Â¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bf52be240dd454992c47b4657462d91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=4, style=ProgressStyle(description_width='initialâ¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   <lambda>                     \n",
      "    0      0.023144   0.119381   0.9168    \n",
      "    1      0.013382   0.116176   0.92008                      \n",
      "    2      0.008749   0.113512   0.92092                       \n",
      "    3      0.006416   0.111717   0.92016                       \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([0.11172]), 0.92016]"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sl = 2_000\n",
    "md = TextClassifierData.from_bow(trn_term_doc3, data.trn_y, val_term_doc3, data.val_y, sl)\n",
    "\n",
    "learner = md.dotprod_nb_learner()\n",
    "learner.fit(0.02, 4, wds=1e-6, cycle_len=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[...](https://youtu.be/XJ_waZlJU8g?t=2764)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Baselines and Bigrams: Simple, Good Sentiment and Topic Classification. Sida Wang and Christopher D. Manning](https://www.aclweb.org/anthology/P12-2018)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "165px",
    "width": "250px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "256px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
